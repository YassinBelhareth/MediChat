from langchain.memory import ConversationBufferMemory
from utils import generate_response

class CustomConversationChain:
    def __init__(self, model, tokenizer):
        """
        Initializes the class with a model, a tokenizer, and memory to manage context.
        """
        self.memory = ConversationBufferMemory()
        self.model = model
        self.tokenizer = tokenizer

    def run(self, user_input):
        """
        Handles user interaction and generates a contextual response.

        Args:
            user_input (str): The user's question.

        Returns:
            str: The response generated by the model.
        """
        # Retrieve the context history
        history = self.memory.buffer

        # Create a prompt with history
        prompt_template = f"""
        You are a helpful and friendly medical assistant. 
        If this is the first interaction, greet the user warmly. 
        If the user has already asked a question, continue without greetings.

        Conversation History:
        {history}

        User: {user_input}
        Assistant:
        """
        # Call generate_response to generate a response
        response = generate_response(prompt_template, self.model, self.tokenizer)

        # Add to memory
        self.memory.save_context({"input": user_input}, {"output": response})

        return response
